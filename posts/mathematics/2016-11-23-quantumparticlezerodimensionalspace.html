<!DOCTYPE html>
<html>
  <head>
    <title> Daniel Barter - quantum particles in a zero dimensional space</title>
    <link href="../../style.css" rel="stylesheet">

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML" async></script>

  </head>
  <body>
    <nav>
<a href="../../"> HOME </a>
<span> / </span>
<a href="../../CV.html"> CV </a>
<span> / </span>
<a href="../../publickey.html"> PUBKEY </a>
<span> / </span>
<a href="../../mix.html"> MIX1010 </a>
<span> / </span>
<a href="../../tableau.html"> TABLEAU </a>
</nav>

<h1 id="quantum-particles-in-a-zero-dimensional-space">Quantum Particles in a Zero Dimensional Space</h1>
<p>Most introductory textbooks on quantum mechanics start by studying a quantum particle in the real line. In this blog post, we are going to think about quantum particles in a finite set.</p>
<h3 id="classical-mechanics-in-a-finite-set">Classical Mechanics in a finite set</h3>
<p>In classical mechanics, particles are modeled as elements of the ambient space which move continuously as a function of time. When the ambient space is a finite set <span class="math inline">\(S\)</span>, the particle <span class="math inline">\(x_t \in S\)</span> is fixed in its initial state <span class="math inline">\(x_0\)</span> for all time. This is because the only continuous maps from the interval to a finite set are constant. If <span class="math inline">\(f : S \to \mathbb{R}\)</span> is an observable, every time we measure <span class="math inline">\(f\)</span>, we record the value <span class="math inline">\(f(x_0)\)</span>. As you can see classical mechanics in a finite set is pretty boring.</p>
<h3 id="statistical-mechanics-in-a-finite-set">Statistical Mechanics in a finite set</h3>
<p>In statistical mechanics, particles are modeled as random variables <span class="math inline">\(X\)</span> taking values in the ambient space <span class="math inline">\(S\)</span>. When <span class="math inline">\(S\)</span> is a finite set, a random variable <span class="math inline">\(X \in S\)</span> is specified by its distribution function <span class="math display">\[p \in \Delta = \{ p_s  \in {\rm maps}(S,\mathbb{R}) : \sum_s p_s = 1, \; p_s \geq 0 \}.\]</span> If <span class="math inline">\(f : S \to \mathbb{R}\)</span> is an observable, when we measure <span class="math inline">\(f\)</span> we record the value <span class="math inline">\(f(s)\)</span> with probability <span class="math inline">\(p_s\)</span>. Now suppose that we actually measure <span class="math inline">\(f\)</span> and record the value <span class="math inline">\(\lambda\)</span>. Then according to Bayes rule, the state <span class="math inline">\(X\)</span> is updated: <span class="math display">\[{\bf P}(X = s | f(X) = \lambda) \propto {\bf P}(f(X) = \lambda | X = s) p_s =
\begin{cases}
p_s &amp; f(s) = \lambda \\
0 &amp; {\rm otherwise}
\end{cases}\]</span> In words, when we record <span class="math inline">\(\lambda\)</span> we truncate the probability of any incompatible state to zero and then scale the resulting vector so it becomes a probability distribution. As you can see, measurement is much more interesting in statistical mechanics. Now lets talk about dynamics. In physics, we want the time evolution of our system to be deterministic. Therefore we can model the passage of time as a function <span class="math display">\[T_t : \Delta \to \Delta\]</span> Moreover, we want the system to be invariant under time translation which implies that <span class="math display">\[T_{t+s} = T_t \circ T_s.\]</span> From the laws of probability theory, we have <span class="math display">\[{\bf P}(T_t X = s') = \sum_s {\bf P}(T_t X = s' | X = s) p_s\]</span> which implies that <span class="math display">\[T_t p = \sum_s p_s t_t \delta_s.\]</span> where <span class="math inline">\(\delta_s\)</span> is the probability distribution concentrated at <span class="math inline">\(s\)</span>. It follows that <span class="math inline">\(T_t\)</span> is linear. Any linear map which preserves <span class="math inline">\(\Delta\)</span> must have all positive real entries and columns summing to one. From Lie theory, we know that <span class="math display">\[T_t = \exp(tH)\]</span> where <span class="math inline">\(H\)</span> is a matrix whose off diagonal entries are positive with columns summing to zero. We call <span class="math inline">\(H\)</span> the Hamiltonian generator for the statistical mechanical system. One of the fundamental concepts in statistical mechanics is entropy: <span class="math display">\[\Theta= \sum_s p_s \log p_s\]</span> We would like to understand how <span class="math inline">\(\Theta\)</span> behaves under time evolution. We have <span class="math display">\[T_t p = p + tHp + O(t^2)\]</span> which implies that the directional derivative is <span class="math display">\[d\Theta(Hp).\]</span> The exterior derivative is <span class="math display">\[d\Theta = \sum_s (\log p_s + 1)dp_s\]</span> which implies that <span class="math display">\[d\Theta(Hp) = (\log p^T) H p.\]</span> Regardless of <span class="math inline">\(H\)</span>, this directional derivative is <span class="math inline">\(0\)</span> when <span class="math inline">\(p\)</span> is uniform. The uniform distribution is the maximum entropy distribution on a finite set, therefore we have proved that if the statistical mechanical system ever reaches maximum entropy, it remains there for the rest of time.</p>
<h3 id="energy-constrained-systems">Energy constrained systems</h3>
<p>In the previous section, we discussed unconstrained statistical mechanical systems. In practice, we only want to study energy constrained systems. This means that we have an energy observable <span class="math inline">\(E : S \to \mathbb{R}\)</span> and we only want to consider states <span class="math inline">\(p\)</span> with a fixed expected energy <span class="math inline">\(e(p) = \mathbb{E}_p(E) = E_0\)</span>. Since we have <span class="math display">\[de = \sum_s E_s dp_s\]</span> the directional derivative for <span class="math inline">\(e\)</span> along time evolution is <span class="math display">\[de(Hp) = E^T Hp\]</span> so the only statistical Hamiltonians which preserve the expected energy are those with <span class="math inline">\(E^T H=0\)</span>. In words, we need the columns of <span class="math inline">\(H\)</span> to be perpendicular to <span class="math inline">\(E\)</span>. It is fun to check that the maximum entropy distribution with a fixed expected energy <span class="math inline">\(E_0\)</span> is exactly the Boltzmann distribution. Also, we have <span class="math display">\[d \Theta\left(HZ e^{-E/k}\right) = \log \left(Z e^{-E/k}\right)^T H p = (\log Z - E/k)^T H p = 0\]</span> So if time evolution preserves expected energy and we are in the Boltzmann state, we stay there for ever. This is one of the standard starting points for statistical mechanics.</p>
<h3 id="quantum-mechanics-in-a-finite-set">Quantum Mechanics in a finite set</h3>
<p>Somewhat surprisingly, the transition from statistical mechanical systems to quantum mechanical systems requires little effort. Now the state of the system is encoded using a wave function. When <span class="math inline">\(S\)</span> is a finite set, the wave function <span class="math inline">\(\chi\)</span> lives in <span class="math display">\[\{ \chi \in {\rm maps}(S,\mathbb{C}) : \langle \chi,\chi \rangle = 1\}\]</span> where <span class="math inline">\(\langle \cdot,\cdot \rangle\)</span> is the standard Hermitian form, and <span class="math inline">\({\rm maps}(S,\mathbb{C})\)</span> is the vector space with basis <span class="math inline">\(S\)</span>. We call <span class="math inline">\(S\)</span> the <strong>computation basis</strong> and denote the basis vectors by <span class="math inline">\(|s\rangle\)</span> as is traditional in quantum mechanics. If <span class="math inline">\(f : S \to \mathbb{R}\)</span> is an observable, when we measure <span class="math inline">\(f\)</span>, we record the value <span class="math inline">\(f(s)\)</span> with probability <span class="math inline">\(\lvert \chi_s \lvert^2\)</span>. If we actually measure <span class="math inline">\(f\)</span> and record <span class="math inline">\(\lambda\)</span>, the wave function collapses according to Bayes rule. The time evolution operator <span class="math inline">\(T_t\)</span> must be unitary to preserve probability, so the infinitesimal generator <span class="math inline">\(H\)</span> is skew Hermitian. This implies that time evolution is invertible. As a result we can talk about generalized observables. Choose an orthonormal basis <span class="math inline">\(\psi_i\)</span> and let <span class="math inline">\(U\)</span> be the unitary operator which takes <span class="math inline">\(\psi_i\)</span> to a computation basis vector. Then we can perform the operation <span class="math display">\[U^{-1} \circ {\rm measurement} \circ U\]</span> The spectral theorem says that a hermitian operator can be diagonalized by a unitary matrix and has real eigenvalues. Therefore, we can identify generalized observables with hermitian operators. This is one of the fundamental principles of quantum mechanics.</p>
<h3 id="the-path-integral">The Path Integral</h3>
<p>Above, the dynamics of our statistical and quantum systems were generated by a Hamiltonian. We can use a path integral to get physical interpretations for the entries in the Hamiltonian. Consider a quantum particle in <span class="math inline">\(\{ 1,2,\dots,n\}\)</span> with time evolution generated by <span class="math inline">\(H\)</span>. Then the time evolution operator is given by <span class="math inline">\(U(t) = e^{tH}\)</span>. The amplitude of transitioning from state <span class="math inline">\(a\)</span> to state <span class="math inline">\(b\)</span> in time <span class="math inline">\(T\)</span> is given by <span class="math display">\[U(T)_{b,a} = [U(T/N)]^N_{b,a}\]</span> If we expand out the matrix multiplication, then the amplitude becomes <span class="math display">\[\sum_{k_{\bullet} = a,k_1,k_2,\dots,k_{N-1},b} U(T/N)_{a,k_{1}} U(T/N)_{k_1,k_2} \cdots U(T/N)_{k_{N-1},b}.\]</span> If we write <span class="math display">\[k_{\bullet} = \underbrace{s_1,s_1,\dots,s_1}_{p_1 + 1},\underbrace{s_2,s_2,\dots,s_2}_{p_2 + 1},\dots,\underbrace{s_J,s_J,\dots,s_J}_{p_J + 1}\]</span> where <span class="math inline">\(s_i \not= s_{i+1}\)</span>, then we can rewrite the sum as <span class="math display">\[
\sum_{J,p_1,\cdots,p_J,s_1,\cdots,s_J} U(T/N)_{s_1,s_1}^{p_1} U(T/N)_{s_1,s_2} U(T/N)_{s_2,s_2}^{p_2} U(T/N)_{s_2,s_3} \cdots U(T/N)_{s_{J-1},s_J} U(T/N)_{s_J,s_J}^{p_J}
\]</span> When <span class="math inline">\(N\)</span> is large, <span class="math inline">\(U(T/N) = I + TH/N\)</span>. Therefore, the sum approximately equals <span class="math display">\[
\sum_{J,p_1,\cdots,p_J,s_1,\cdots,s_J}  \left(1 + \frac{T H_{s_1 s_1}}{N}\right)^{p_1}\frac{T}{N}H_{s_1 s_2} \left(1 + \frac{T H_{s_2 s_2}}{N}\right)^{p_2}\frac{T}{N}H_{s_2 s_3} \cdots  \frac{T}{N}H_{s_{J-1} s_{J}}\left(1 + \frac{T H_{s_J s_J}}{N}\right)^{p_J}
\]</span> <span class="math display">\[
\approx \sum_{J,p_1,\cdots,p_J,s_1,\cdots,s_J} (T/N)^{J-1}e^{p_1 T H_{s_1 s_1} / N} H_{s_1 s_2}e^{p_2 T H_{s_2 s_2} / N} H_{s_2 s_3} \cdots H_{s_{J-1} s_J}e^{p_J T H_{s_J s_J} / N}
\]</span> Define <span class="math inline">\(X(a,b)\)</span> to be the space of functions <span class="math inline">\(f : [0,T] \to \{ 1,2,\dots,n\}\)</span> with <span class="math inline">\(f(0) = a\)</span>, <span class="math inline">\(f(T)=b\)</span> and a finite number of discontinuities. Define <span class="math inline">\(A : X(a,b) \to \mathbb{C}\)</span> by the equation <span class="math display">\[f = [ \underbrace{\quad s_1 \quad}_{t_1} | \underbrace{\quad s_2 \quad}_{t_2} | \cdots | \underbrace{\quad s_J \quad}_{t_J}] \mapsto e^{t_1 H_{s_1 s_1}} H_{s_1 s_2} e^{t_2 H_{s_2 s_2}} \cdots H_{s_{J-1} s_J}e^{t_J H_{s_J s_J}}.\]</span> Then the above sum is exactly a Riemann sum for <span class="math inline">\(A\)</span>. If we let <span class="math inline">\(N \to \infty\)</span>, then we have <span class="math display">\[U(T)_{b,a} = \int_{X(a,b)} A(f) df.\]</span> In words, the transition amplitude is a sum over all possible paths weighted by <span class="math inline">\(A(f)df\)</span>. we can interpret <span class="math inline">\(A(f)\)</span> as the amplitude that the path <span class="math inline">\(f\)</span> occoured. The term <span class="math inline">\(e^{t H_{s_is_i}}\)</span> is the amplitude that the particle stays in state <span class="math inline">\(s_i\)</span> for time <span class="math inline">\(t\)</span>. We can interpret <span class="math inline">\(H_{s_i,s_j}\)</span> as the transition amplitude from state <span class="math inline">\(s_i\)</span> to state <span class="math inline">\(s_j\)</span>.</p>


  </body>
</html>
