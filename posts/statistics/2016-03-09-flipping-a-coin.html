<!DOCTYPE html>
<html>
  <head>
    <title> Daniel Barter - flipping a coin</title>
    <link href="../../style.css" rel="stylesheet">

    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    <script src="mix.js"></script>

  </head>
  <body>

      <nav>
        <a href="../../"> HOME </a>
        <span> / </span>
        <a href="../../CV.html"> CV </a>
        <span> / </span>
        <a href="../../blog.html"> BLOG </a>
        <span> / </span>
        <a href="mix.html"> MIX1010 </a> 
      </nav>

    <h1 id="flipping-a-coin">Flipping a Coin</h1>
<p>A few weeks ago, I was exposed to my first large data set. It made me want to learn some statistical theory.</p>
<p>Suppose that you have a coin. You flip it <span class="math inline">\(n\)</span> times and record the number of heads. What can you say about the coin?</p>
<h3 id="the-frequentist-approach">The Frequentist Approach</h3>
<p>The coin has some fixed unknown probability <span class="math inline">\(\phi\)</span> of landing heads. Consider the following experiment: flip the coin <span class="math inline">\(n\)</span> times. Let <span class="math inline">\(H\)</span> be the number of heads. We want to estimate bounds on <span class="math inline">\(\phi\)</span>. One way to do this is by finding functions <span class="math inline">\(f,g\)</span> such that <span class="math display">\[{\bf P}(\phi \in [f(H),g(H)]) \geq 0.95 \qquad \text{for all $\phi$}\]</span> In English, this says that we are 95% sure that <span class="math inline">\(\phi\)</span> lies between <span class="math inline">\(f(H)\)</span> and <span class="math inline">\(g(H)\)</span>. We call <span class="math inline">\([f(H),g(H)]\)</span> a <strong>confidence interval</strong>. The value <span class="math inline">\(0.95\)</span> was chosen to make things more concrete. It follows from <a href="https://en.wikipedia.org/wiki/Hoeffding's_inequality">Hoeffding's Inequality</a> that if <span class="math inline">\(X_1,\dots,X_n \sim {\rm Bernoulli}(\phi)\)</span> are independent, then for any <span class="math inline">\(\epsilon &gt; 0\)</span>, we have <span class="math display">\[{\bf P}\left( \left| \frac{\sum X_i}{n} - \phi  \right| &gt; \epsilon \right) \leq 2 
e^{-2n \epsilon^2}\]</span> which implies that for any <span class="math inline">\(\epsilon &gt; 0\)</span>, <span class="math display">\[{\bf P} \left( \phi \in \left[ \frac{\sum X_i}{n} - \epsilon, \frac{\sum X_i}{n} + 
\epsilon \right] \right) \geq 1 - 2 e^{-2n\epsilon^2}.\]</span> If we set <span class="math inline">\(\epsilon = \sqrt{log(40)/2n}\)</span>, then we get <span class="math display">\[{\bf P} \left( \phi \in \left[ \frac{\sum X_i}{n} - \sqrt{log(40)/2n}, \frac{\sum 
X_i}{n} + \sqrt{log(40)/2n} \right] \right) \geq 0.95.\]</span> Since <span class="math inline">\(H = \sum X_i\)</span>, we are 95% sure that <span class="math inline">\(\phi\)</span> lies between <span class="math inline">\(H / n - \sqrt{log(40)/2n}\)</span> and <span class="math inline">\(H / n + \sqrt{log(40)/2n}\)</span>. There are many other ways to calculate confidence intervals. We have chosen to use Hoeffding's Inequality because it does not place any restrictions on the sample size.</p>
<h3 id="the-bayesian-approach">The Bayesian Approach</h3>
<p>Write <span class="math inline">\(\Phi\)</span> for the probability that the coin lands heads. Since we do not know the value of <span class="math inline">\(\Phi\)</span>, we consider it a random variable. We have no prior information about <span class="math inline">\(\Phi\)</span>, so our initial guess is that <span class="math inline">\(\Phi\)</span> is uniformly distributed: the probability density function is <span class="math inline">\(f_{\Phi}(\phi) = 1\)</span>.</p>
<div class="figure">
<img src="../../img/2016-03-09-predist.PNG" alt />

</div>
<p>Suppose that we flip the coin <span class="math inline">\(n\)</span> times and observe <span class="math inline">\(r\)</span> heads. Call this event <span class="math inline">\(K\)</span>. We now have more information, so we should update our probability density function. We do this using <a href="https://en.wikipedia.org/wiki/Bayes'_theorem">Bayes law</a>. Let <span class="math inline">\(h\)</span> be very small. Then</p>
<p><span class="math display">\[{\bf P}(\phi \leq \Phi \leq \phi + h | K) = \frac{{\bf P}(K | \phi \leq \Phi 
\leq \phi + h) {\bf P}(\phi \leq \Phi \leq \phi + h)}{{\bf P}(K)}.\]</span></p>
<p>We can compute each term on the right hand side as follows: <span class="math display">\[{\bf P}(K | \phi \leq \Phi \leq \phi + h) = \phi^r (1 - \phi)^{n-r}\]</span> <span class="math display">\[{\bf P}(\phi \leq \Phi \leq \phi + h) =f_{\Phi}(\phi)h = h\]</span> <span class="math display">\[{\bf P}(K) = \sum_i {\bf P}(K | \phi_i \leq \Phi \leq \phi_{i+1}) {\bf P}(\phi_i 
\leq \Phi \leq \phi_{i+1}) = \sum_i \phi_i^r (1 - \phi_i)^{n-r} \Delta \phi_i = 
\int_0^1 \phi^r(1 - \phi)^{n-r} d \phi\]</span> This implies that <span class="math display">\[f_{\Phi | K}(\phi) 
  = \lim_{h \to 0} \frac{{\bf P}(\phi \leq \Phi \leq \phi + h | K)}{h} 
  = \frac{\phi^r(1-\phi)^{n-r}}{\int_0^1 \phi^r(1 - \phi)^{n-r} d \phi}\]</span> which looks like</p>
<div class="figure">
<img src="../../img/2016-03-09-postdist.PNG" alt />

</div>
<p>As you can see, the new distribution is more concentrated around <span class="math inline">\(r/n\)</span>.</p>
<h3 id="discussion">Discussion</h3>
<p>As a mathematician, it is not my place to argue for either the Frequentist or Bayesian approach to this problem. As a human being, I favor the Bayesian approach. Less mental gymnastics is required to interpret pictures of density functions than is required to interpret statements like &quot;we are 95% sure that <span class="math inline">\(\phi\)</span> is between <span class="math inline">\(f(H)\)</span> and <span class="math inline">\(g(H)\)</span>&quot;. Another benefit of the Bayesian approach is that it suggests how to store unknown values in a computer: Consider them as random variables and store the density function. As you learn more information, the density function is updated.</p>

  </body>
</html>
