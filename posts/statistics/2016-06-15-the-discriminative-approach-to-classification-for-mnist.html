<!DOCTYPE html>
<html>
  <head>
    <title> Daniel Barter - the discriminative approach to classification for mnist</title>
    <link href="../../style.css" rel="stylesheet">

    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  </head>
  <body>

      <nav>
        <a href="../../"> HOME </a>
        <span> / </span>
        <a href="../../CV.html"> CV </a>
        <span> / </span>
        <a href="../../blog.html"> BLOG </a>
        <span> / </span>
        <a href="https://github.com/danielbarter"> GITHUB </a> 
      </nav>

    <h1 id="the-discriminative-approach-to-classification-for-mnist">The Discriminative Approach to Classification for MNIST</h1>
<p>In the <a href="2016-06-08-naive-bayes-classification-and-mnist-database.html">last post</a> we described the simplest possible generative approach to classifying digits in the MNIST database. In this post, we are going to explore the discriminative approach to classification.</p>
<h3 id="the-model">The Model</h3>
<p>Let <span class="math inline">\(\mathcal{D} = \left\{ y^n \in S,x^n \in \mathbb{R}^f \right\}_{n=1,\dots,N}\)</span> be a data set. For notational convenience, we embed <span class="math inline">\(S\)</span> into Euclidean space: <span class="math display">\[S = \{ s_1,\dots,s_c \} \hookrightarrow \mathbb{R}^c \quad s_i \mapsto e_i.\]</span> Given a parameter space <span class="math inline">\(\Theta\)</span> and functions <span class="math inline">\(f_1,\dots,f_c : \Theta \times \mathbb{R}^f \to (0,1)\)</span>, we define the following probability distribution: <span class="math display">\[p(y | \theta,x) = \prod_i f_i(\theta,x)^{y_i}\]</span> Given a prior distribution <span class="math inline">\(p(\theta)\)</span> over <span class="math inline">\(\Theta\)</span>, Bayes rule implies that <span class="math display">\[\log p(\theta | \mathcal{D}) = \log p(\theta) + \sum_{n,i} y^n_i \log 
f_i(\theta,x^n).\]</span> We can choose parameters given data using maximum likelihood estimation. More precisely, we want to minimize the following expression: <span class="math display">\[-\sum_{n,i} y^n_i \log f_i(\theta,x^n)\]</span> Intuitively, we are choosing parameters which minimize the <a href="https://en.wikipedia.org/wiki/Cross_entropy">cross entropy</a> between the predicted distribution on <span class="math inline">\(y\)</span> and the observed distribution on <span class="math inline">\(y\)</span>. Minimizing the cross entropy is only half the story. We also need to choose the parameter space <span class="math inline">\(\Theta\)</span> and functions <span class="math inline">\(f_1,\dots,f_c\)</span>. To keep things simple, we shall take <span class="math display">\[(f_1,\dots,f_c)(W,b,x) = {\rm softmax}(Wx + b)\]</span> where <span class="math inline">\(W\)</span> is a <span class="math inline">\(c \times f\)</span> matrix, <span class="math inline">\(b \in \mathbb{R}^c\)</span> and <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a> transforms the output into a probability distribution.</p>
<h3 id="the-implementation">The implementation</h3>
<p>Unlike for naive Bayes classification, implementing the above model from scratch is not trivial. This is because we need to minimize a smooth function defined on a <span class="math inline">\(c(f+1)\)</span> dimensional manifold. This type of problem shows up a lot in machine learning, and there are many libraries to choose from for solving this problem. One such library is <a href="https://www.tensorflow.org/">TensorFlow</a>. Indeed, the <a href="https://www.tensorflow.org/versions/r0.9/tutorials/mnist/beginners/index.html">first example</a> in the TensorFlow tutorial is implementing the model we described above! It is able to classify digits from the test set with about 90% accuracy. This is a 5% improvement over naive Bayes classification, but we need to use more sophisticated numerical techniques to compute the maximum likelihood estimate.</p>

  </body>
</html>
