<!DOCTYPE html>
<html>
  <head>
    <title> Daniel Barter - naive bayes classification and the mnist database</title>
    <link href="../../style.css" rel="stylesheet">

    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  </head>
  <body>

      <nav>
        <a href="../../"> HOME </a>
        <span> / </span>
        <a href="../../CV.html"> CV </a>
        <span> / </span>
        <a href="../../blog.html"> BLOG </a>
        <span> / </span>
        <a href="../../mix.html"> MIX1010 </a>
        <span> / </span>
        <a href="tableau.html"> TABLEAU </a>
      </nav>

    <h1 id="naive-bayes-and-the-mnist-database">Naive Bayes and the MNIST Database</h1>
<p>The <a href="http://yann.lecun.com/exdb/mnist/">MNIST database</a> consists of handwritten digits stored as <span class="math inline">\(28 \times 28\)</span> bit maps. In this post, we are going to use the database to train a naive Bayesian classifier.</p>
<h3 id="the-model">The Model</h3>
<p>Our model has the following random variables:</p>
<ul>
<li><span class="math inline">\(c \in \{ 0,1,2,\dots,9\}\)</span>: the digit label. Each bitmap in the data set is labeled by the digit it is trying to represent.</li>
<li><span class="math inline">\(x \in \{0,1\}^{28 \times 28}\)</span>: the bit map. Although the database consists of Gray scale bit maps, we have transformed them into black and white. This reduces the number of parameters in our model.</li>
<li><span class="math inline">\(\theta \in [0,1]^{28 \times 28 \times 10}\)</span>: the activation probability. The random variable <span class="math inline">\(\theta_{ic}\)</span> represents the probability that bit <span class="math inline">\(i\)</span> is turned on in a bitmap which is labeled <span class="math inline">\(c\)</span>.</li>
<li><span class="math inline">\(\pi \in [0,1]^{10}\)</span>: the label probability. The random variable <span class="math inline">\(\pi_c\)</span> represents the probability that a given bitmap has label <span class="math inline">\(c\)</span>.</li>
</ul>
<p>We are using a naive Bayesian model. Therefore</p>
<ul>
<li><span class="math inline">\(x_i \perp x_j | c,\theta,\pi\)</span>: This is what we mean by naive Bayesian. If the label is fixed, then different pixels are independent.</li>
</ul>
<p>The joint distribution is given by</p>
<p><span class="math display">\[p(c,x,\theta,\pi) = p(c,x | \theta,\pi) p(\theta,\pi) = p(x | c,\theta,\pi) p(c | 
\theta,\pi) p(\theta,\pi) = p(\theta,\pi) \pi_c \prod_{i} p(x_i \lvert c,\theta)\]</span></p>
<p>The MNIST database is <span class="math inline">\(\mathcal{D} = \left\{ c^{(n)},x^{(n)} \right\}_{n=1,\dots,N}\)</span> and we are interested in computing the distribution <span class="math inline">\(p(c | x, \mathcal{D})\)</span>. The joint posterior distribution is <span class="math display">\[p(c,x,\theta,\pi | \mathcal{D}) = p(c,x|\theta,\pi,\mathcal{D}) 
p(\theta,\pi|\mathcal{D}) = p(c,x | \theta,\pi) p(\theta,\pi|\mathcal{D})\]</span> We use the prior distribution <span class="math display">\[p(\theta,\pi) = {\rm Dirichlet}(\pi,1) \prod_{i,c} {\rm Beta}(\theta_{i,c},1,1).\]</span> Then the posterior distribution is <span class="math display">\[p(\theta,\pi \lvert \mathcal{D}) = {\rm Dirichlet}(\pi,N_c+1) \prod_{i,c} 
{\rm Beta}(\theta_{i,c},N_{ic} + 1,N_c - N_{ic} + 1)\]</span> where</p>
<ul>
<li><span class="math inline">\(N_c\)</span>: classifier frequency. The number of bitmaps labeled <span class="math inline">\(c\)</span> in <span class="math inline">\(\mathcal{D}\)</span></li>
<li><span class="math inline">\(N_{ic}\)</span>: pixel frequency. The number of times the <span class="math inline">\(i\)</span>th bit is turned on when the label is <span class="math inline">\(c\)</span>.</li>
</ul>
<p>Since <span class="math inline">\(\mathcal{D}\)</span> is a large data set <span class="math inline">\((N=60,000)\)</span>, we can approximate the posterior <span class="math inline">\(p(\theta,\pi \lvert \mathcal{D})\)</span> as a dirac measure supported at its mean: <span class="math display">\[p(\theta,\pi \lvert \mathcal{D}) = \delta_{(\widehat{\theta},\widehat{\pi})} \quad 
 \widehat{\theta}_{ic} = \frac{N_{ic} + 1}{N_c + 1} \quad \widehat{\pi}_{c} = 
\frac{N_c + 1}{N + 1}\]</span> Therefore <span class="math display">\[p(c,x | \mathcal{D}) = \int p(c,x | \theta,\pi) p(\theta,\pi \lvert \mathcal{D}) = 
p(c,x| \widehat{\theta},\widehat{\pi})\]</span> which implies that <span class="math display">\[p(c \lvert x, \mathcal{D}) \varpropto p(x | c,\widehat{\theta},\widehat{\pi}) 
\widehat{\pi}_c\]</span></p>
<h3 id="the-implementation">The implementation</h3>
<p>The MNIST bit maps are in gray scale. Firstly, we flatten them to black and white and translate the raw byte files into CSV files <a href="https://github.com/danielbarter/personal_website_code/tree/master/blog_notebooks/Niave_Bayes_classification_MNIST/mnistclean">using Haskell</a>. We then implement the above model <a href="https://github.com/danielbarter/personal_website_code/blob/master/blog_notebooks/Niave_Bayes_classification_MNIST/Niave_Bayes_Classification_MNIST_MLE.ipynb">in Python</a>. The trained NBC correctly identifies <span class="math inline">\(85\%\)</span> of the test set.</p>

  </body>
</html>
