<!DOCTYPE html>
<html>
  <head>
    <title> Daniel Barter - entropy and fisher information</title>
    <link href="../../style.css" rel="stylesheet">

    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    <script src="mix.js"></script>

  </head>
  <body>

      <nav>
        <a href="../../"> HOME </a>
        <span> / </span>
        <a href="../../CV.html"> CV </a>
        <span> / </span>
        <a href="../../blog.html"> BLOG </a>
        <span> / </span>
        <a href="../../mix.html"> MIX1010 </a> 
      </nav>

    <h1 id="entropy-and-fisher-information">Entropy and Fisher Information</h1>
<p><em>No one really knows what entropy really is....</em></p>
<p>This famous quote is due to Von Neumann. In this dense post, we are going to record some of the places where entropy shows up in probability theory and statistics.</p>
<h3 id="information">Information</h3>
<p>Suppose that we have a device which has a finite set of possible configurations <span class="math inline">\(C\)</span>. Suppose that we also have a function <span class="math inline">\(X : C \to S\)</span> where <span class="math inline">\(S\)</span> is another finite set. If we have no information about the current configuration of the device, then <span class="math inline">\(X\)</span> is a random variable with distribution <span class="math display">\[p(s) = \frac{ \lvert X^{-1}(s) \lvert}{\lvert C \lvert}.\]</span> If we measure the random variable <span class="math inline">\(X\)</span> and we see <span class="math inline">\(s\)</span>, then we know that the configuration must be in the subset <span class="math inline">\(X^{-1}(s)\)</span>. If we are encoding the possible configurations in binary, then we just learned the value of <span class="math display">\[\log \lvert C \lvert - \log \lvert X^{-1}(s)  \lvert = \log \frac{1}{p(s)}\]</span> bits. Motivated by this, we call this number the <strong>information content</strong> of the measurement <span class="math inline">\(s\)</span>.</p>
<h3 id="source-coding-theorem">Source coding theorem</h3>
<p>Suppose that <span class="math inline">\(S\)</span> is a finite set and <span class="math inline">\(X_1,X_2,\dots,X_n \in S\)</span> are independent identically distributed random variables with distribution <span class="math inline">\(p(s)\)</span>. The random variable <span class="math inline">\((X_1,\dots,X_n) \in S^n\)</span> has distribution <span class="math inline">\(p(s_1) p(s_2) \cdots p(s_n)\)</span>. We want to understand how localized this distribution is. The trick is to apply the weak law of large numbers in the following sneaky way: <span class="math display">\[\forall \epsilon &gt; 0 \quad \lim_{n \to \infty} {\bf P}\left( \left\lvert \frac{\log 
p(X_1) \cdots p(X_n) }{n} - {\bf E}(\log p(X)) \right\lvert &lt; \epsilon \right) = 1\]</span> We define <span class="math display">\[H(X) = \sum_{s \in S} p(s) \log \frac{1}{p(s)} = - {\bf E}(\log p(X))\]</span> This quantity is called the <strong>entropy</strong> of the random variable <span class="math inline">\(X\)</span>. We have <span class="math display">\[\forall \epsilon &gt; 0 \quad \lim_{n \to \infty}  
{\bf P} \left( 2^{-n(H+\epsilon)} &lt; \prod_i p(X_i) &lt; 2^{-n(H-\epsilon)} \right) = 1.\]</span> If we define <span class="math display">\[A_n^{\epsilon} = \left\{ (s_1,\dots,s_n) \in S^n : 2^{-n(H+\epsilon)} &lt; \prod_i 
p(s_i) &lt; 2^{-n(H-\epsilon)}  \right\}\]</span> then we have <span class="math display">\[\forall \epsilon &gt; 0 \quad \lim_{n \to \infty} {\bf P}( (X_1,\dots,X_n) \in 
A_n^{\epsilon} 
) = 1\]</span> We call <span class="math inline">\(A_n^{\epsilon}\)</span> the <strong>typical set</strong>. Its size is less than <span class="math inline">\(2^{n(H+\epsilon)}\)</span>. Informally, with high probability, we can record the string <span class="math inline">\((X_1,\dots,X_n)\)</span> in <span class="math inline">\(n(H+\epsilon)\)</span> bits. Even more informally, if <span class="math inline">\(\epsilon\)</span> is small and <span class="math inline">\(n\)</span> is large, the set <span class="math inline">\(A_n^{\epsilon}\)</span> has <span class="math inline">\(2^{n H}\)</span> elements and each element has probability <span class="math inline">\(2^{-nH}\)</span>. If we run through the proof again using the central limit theorem instead of the weak law of large numbers, we can get quantitative information about the typical set.</p>
<h3 id="kl-divergence-and-maximum-likelihood-estimation">KL-divergence and maximum likelihood estimation</h3>
<p>Suppose that <span class="math inline">\(x^1,x^2,...,x^N\)</span> are independent, identically distributed samples from a probability distribution <span class="math inline">\(p(x)\)</span>. Suppose we have a parameterized model <span class="math inline">\(p(x|\theta)\)</span> for the data. Then the maximum likelihood is given by <span class="math display">\[\theta_{\rm ML} = {\rm argmax}_{\theta} \frac{1}{N} \sum_{n=1}^N \log q(x^i | \theta).\]</span> If <span class="math inline">\(N\)</span> is large, then by the weak law of large numbers, with high probability, we have <span class="math display">\[\theta_{\rm ML} = {\rm argmax}_{\theta} {\bf E}_p(\log q(x | \theta)) 
= {\rm argmin}_{\theta} - {\bf E}_p(\log q(x | \theta))\]</span> Therefore, if <span class="math inline">\(N\)</span> is large, then maximum likelihood estimation is the same as minimizing the cross entropy <span class="math display">\[H(p,q) = - {\bf E}_p(\log q(x|\theta)) = \sum_x p(x) \log \frac{1}{q(x|\theta)}\]</span> We define <span class="math display">\[D(p || q) = H(p,q) - H(p) = \sum_x p(x) \log \frac{p(x)}{q(x|\theta)}.\]</span> Then maximum likelihood estimation is the same as minimizing <span class="math inline">\(D(p || q(\cdot | \theta))\)</span> as a function of <span class="math inline">\(\theta\)</span>. As a function of <span class="math inline">\(q\)</span>, <span class="math inline">\(D(p || q)\)</span> is non negative, convex and minimized when <span class="math inline">\(p=q\)</span>.</p>
<h3 id="parametric-inference-and-fisher-information">Parametric inference and Fisher information</h3>
<p>Once again, suppose that <span class="math inline">\(S\)</span> is a finite set and <span class="math inline">\(X \in S\)</span> is a random variable. Suppose that <span class="math inline">\(p(s,\theta)\)</span> is a family of density functions for <span class="math inline">\(X\)</span> parameterized by <span class="math inline">\(\theta \in \Theta\)</span> where <span class="math inline">\(\Theta\)</span> is a manifold. If <span class="math inline">\(f : \Theta \to \mathbb{R}\)</span> is a smooth function, an unbiased estimator for <span class="math inline">\(f\)</span> is a function <span class="math inline">\(e : S \to \Theta\)</span> such that <span class="math display">\[{\bf E}(f(e(X))) = f(\theta).\]</span> Unpacking this equation, we get <span class="math display">\[\sum_{s \in S}(f(e(s)) - f(\theta))p(s,\theta) = 0.\]</span> If we take the exterior derivative, then <span class="math display">\[-\sum_s p(s,\theta) df + \sum_s (f(e(s))-f(\theta)) d p(s,\theta) = 0\]</span> which implies that <span class="math display">\[\sum_s (f(e(s))-f(\theta)) d \log p(s,\theta) \cdot p(s,\theta) = df.\]</span> Using the Cauchy-Schwartz inequality and <span class="math inline">\({\bf E}(d \log p(s,\theta)) = 0\)</span>, we have <span class="math display">\[{\bf V}(f(e(X)) {\bf V}(d \log p(s,\theta)) \geq df \otimes df.\]</span> Let us define <span class="math inline">\(g = {\bf E}(d \log p(s,\theta) \otimes d \log p(s,\theta)) = {\bf V}(d \log p(s,\theta))\)</span>. This symmetric 2-tensor is positive definite and in many cases, non-degenerate. We call it the <strong>Fisher information metric</strong>. In terms of <span class="math inline">\(g\)</span>, we have <span class="math display">\[{\bf V}(f(e(X))) \geq \frac{df \otimes df}{g}\]</span> for all tangent vectors in <span class="math inline">\(T_{\theta} \Theta\)</span>. This is called the Cramer-Rao inequality. Let us inspect <span class="math inline">\(g\)</span> in the coordinate chart <span class="math inline">\(\theta^1,\dots,\theta^d\)</span>. We have <span class="math display">\[g = {\bf E}\left( \frac{\partial \log p(s,\theta)}{\partial \theta^i} 
\frac{\partial \log p(s,\theta)}{\partial \theta^j} \right) d \theta^i \otimes 
d\theta^j.\]</span> The equation <span class="math inline">\({\bf E}(d \log p(s,\theta))=0\)</span> implies that <span class="math display">\[{\bf E} \left( \frac{\partial \log p(s,\theta)}{\partial \theta^i} \right) = \sum_s 
\frac{\partial \log p(s,\theta)}{\partial \theta^i} p(s,\theta) = 0.\]</span> If we take the exterior derivative, then we get <span class="math display">\[g = -{\bf E}\left( \frac{\partial^2 \log p(s,\theta)}{\partial \theta^i \partial 
\theta^j} \right) d \theta^i \otimes d\theta^j.\]</span> This gives us a coordinate independent formula for the expectation of the Hessian of <span class="math inline">\(\log p(s,\theta)\)</span>. If we take a Taylor expansion for <span class="math inline">\(\log p(s,\theta)\)</span> and apply <span class="math inline">\({\bf E}\)</span> to each side, then we get <span class="math display">\[\sum_s \log \frac{p(s,\theta)}{p(s,\theta + tV)} 
p(s,\theta) = \frac{1}{2} g(tV,tV)\]</span> for small <span class="math inline">\(t\)</span>. Once again, we see the relative entropy appearing and the Fisher information measures how the relative entropy changes for small motions in the parameter space.</p>
<h3 id="jeffreys-prior">Jeffrey's Prior</h3>
<p>The Fisher information naturally appears when you are interested in the variance of unbiased estimators in parametric inference. It is also important in Bayesian inference. We can use it to produce a distribution on <span class="math inline">\(\Theta\)</span> which is independent of coordinate system. This distribution is called <strong>Jeffery's prior</strong>.</p>
<p>Why is this important? Sometimes, you end up in a situation where you want to do Bayesian inference, but you don't have a prior distribution. In situations like this, people will often choose a uniform prior distribution on the parameters. This is ambiguous, because the phrase <em>uniform prior</em> depends on how you coordinatized your parameters. A prior which is uniform in one coordinate system will probably not be uniform in a different coordinate system. Jeffery's prior is an unambiguous prior which you can use in this situation.</p>
<p>Suppose that the parameter space <span class="math inline">\(\Theta\)</span> is oriented and <span class="math inline">\(d\)</span>-dimensional. The Fisher metric <span class="math inline">\(g\)</span> induces a metric on the line bundle <span class="math inline">\(\wedge^d T\Theta\)</span>. The formula is <span class="math display">\[g(V_1 \wedge \dots \wedge V_d, W_1 \wedge \dots \wedge W_d) = \det(g(V_i,W_j))\]</span> Since <span class="math inline">\(\Theta\)</span> is oriented, the line bundle <span class="math inline">\(\wedge^d T \Theta\)</span> is trivial and we can define a global section <span class="math inline">\(E\)</span> by the formula <span class="math inline">\(g(E,E) = 1\)</span>. Choose coordinates <span class="math inline">\(\theta^1,\dots,\theta^d\)</span> on <span class="math inline">\(\Theta\)</span>. Then <span class="math display">\[g \left( \frac{\partial}{\partial \theta^1} \wedge \cdots \wedge 
\frac{\partial}{\partial \theta^d},\frac{\partial}{\partial \theta^1} \wedge \cdots 
\wedge \frac{\partial}{\partial \theta^d}  \right) = \det(g_{ij})\]</span> Therefore <span class="math display">\[E = \frac{1}{\sqrt{\det(g_{ij})}} \frac{\partial}{\partial \theta^1} \wedge \cdots
\wedge \frac{\partial}{\partial \theta^d}\]</span> The dual section of <span class="math inline">\(\wedge^d T^* \Theta\)</span> is given in coordinates by <span class="math display">\[J = \sqrt{\det(g_{ij})} d \theta^1 \wedge \dots \wedge d \theta^d.\]</span> If we normalize <span class="math inline">\(J\)</span> so that <span class="math inline">\(\int_{\Theta} J = 1\)</span>, then we get Jeffery's Prior.</p>

  </body>
</html>
