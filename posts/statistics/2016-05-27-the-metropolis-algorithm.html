<!DOCTYPE html>
<html>
  <head>
    <title> Daniel Barter - the metropolis algorithm</title>
    <link href="../../style.css" rel="stylesheet">

    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  </head>
  <body>

      <nav>
        <a href="../../"> HOME </a>
        <span> / </span>
        <a href="../../CV.html"> CV </a>
        <span> / </span>
        <a href="../../blog.html"> BLOG </a>
        <span> / </span>
        <a href="https://github.com/danielbarter"> GITHUB </a> 
      </nav>

    <h1 id="the-metropolis-algorithm">The Metropolis Algorithm</h1>
<p>Suppose that <span class="math inline">\(S\)</span> is a set, <span class="math inline">\(\{ p(s \lvert \theta) : \theta \in \Theta\}\)</span> is a family of distributions on <span class="math inline">\(S\)</span> and <span class="math inline">\(p(\theta)\)</span> is a prior distribution on <span class="math inline">\(\Theta\)</span>. If we observe <span class="math inline">\(s \in S\)</span>, then the posterior distribution for <span class="math inline">\(\theta\)</span> is given by <span class="math display">\[p(\theta \lvert s) \propto p(s \lvert \theta) p(\theta).\]</span> The posterior distribution captures the information we learn about <span class="math inline">\(\theta\)</span> from observing <span class="math inline">\(s\)</span>. Therefore, computing <span class="math inline">\(p(\theta \lvert s)\)</span> is of central importance. If <span class="math inline">\(p(s \lvert \theta)\)</span> is simple and <span class="math inline">\(p(\theta)\)</span> is a <a href="https://en.wikipedia.org/wiki/Conjugate_prior">conjugate prior</a>, then computing the posterior distribution is easy. In practice, the statistical model <span class="math inline">\(\{ p(s \lvert \theta) : \theta \in \Theta \}\)</span> can be very complex. In this post, we shall describe the <a href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm">Metropolis algorithm</a> for sampling from the posterior distribution <span class="math inline">\(p(\theta \lvert s)\)</span>. Refined versions of this algorithm drive many Bayesian software libraries like <a href="https://pymc-devs.github.io/pymc/">pyMC</a> and <a href="https://en.wikipedia.org/wiki/WinBUGS">winBUGS</a>. To make the mathematics easier, we shall assume that <span class="math inline">\(S\)</span> and <span class="math inline">\(\Theta\)</span> are finite sets. In practice, <span class="math inline">\(\Theta\)</span> is usually a high dimensional manifold.</p>
<h3 id="markov-chains">Markov Chains</h3>
<p>A <a href="https://en.wikipedia.org/wiki/Markov_chain">Markov chain</a> on <span class="math inline">\(\Theta\)</span> is defined by a matrix <span class="math inline">\(K(\alpha,\beta)\)</span> with <span class="math inline">\(K(\alpha,\beta) \geq 0\)</span> and <span class="math inline">\(\sum_{\beta} K(\alpha,\beta) =1\)</span>. The matrix <span class="math inline">\(K\)</span> specifies a random walk on <span class="math inline">\(\Theta\)</span>: If you are at <span class="math inline">\(\alpha\)</span>, step to <span class="math inline">\(\beta\)</span> with probability <span class="math inline">\(K(\alpha,\beta)\)</span>. If you are at <span class="math inline">\(\alpha\)</span>, then the probability that you arrive at <span class="math inline">\(\beta\)</span> after <span class="math inline">\(n\)</span> steps is <span class="math inline">\(K^n(\alpha,\beta)\)</span>. We call <span class="math inline">\(\pi(\alpha)\)</span> a <em>stationary distribution</em> for the Markov chain if <span class="math display">\[\sum_{\alpha} \pi(\alpha) K(\alpha,\beta) = \pi(\beta).\]</span> If for some <span class="math inline">\(N\)</span>, every entry of <span class="math inline">\(K^N\)</span> is positive (the Markov chain is <em>connected</em>), then the Markov chain has a unique stationary distribution <span class="math inline">\(\pi\)</span> and <span class="math display">\[K^n(\alpha,\beta) \to \pi(\beta).\]</span></p>
<h3 id="the-metropolis-algorithm-1">The Metropolis Algorithm</h3>
<p>Let <span class="math inline">\(J\)</span> be a Markov matrix on <span class="math inline">\(\Theta\)</span> and <span class="math inline">\(\pi\)</span> a probability distribution on <span class="math inline">\(\Theta\)</span>. Set <span class="math inline">\(A(\alpha,\beta) = \pi(\beta) J(\beta,\alpha) / \pi(\alpha) J(\alpha,\beta)\)</span>. Then we can define a new Markov matrix by <span class="math display">\[
K(\alpha,\beta) = \begin{cases}
J(\alpha,\beta) &amp; \alpha \not= \beta, A(\alpha,\beta) \geq 1 \\
J(\alpha,\beta) A(\alpha,\beta) &amp; \alpha\not= \beta, A(\alpha,\beta) &lt; 1 \\
J(\alpha,\beta) + \sum_{\gamma:A(\alpha,\gamma) &lt; 1} J(\alpha,\gamma) (1 - 
A(\alpha,\gamma)) &amp; \alpha=\beta
\end{cases}
\]</span> This formula looks complicated, but you can check that <span class="math inline">\(\pi(\alpha) K(\alpha,\beta) = \pi(\beta) K(\beta,\alpha)\)</span> which implies that <span class="math inline">\(\pi\)</span> is a stationary distribution for <span class="math inline">\(K\)</span>. If we take <span class="math inline">\(\pi(\alpha) = p(\alpha \lvert s)\)</span>, the posterior distribution, then <span class="math display">\[A(\alpha,\beta) = \frac{p(\beta|s) J(\beta,\alpha)}{p(\alpha|s) J(\alpha,\beta)} = 
\frac{p(s|\beta) p(\beta) J(\beta,\alpha)}{p(s|\alpha) p(\alpha) J(\alpha,\beta)}\]</span> Therefore, the Markov matrix <span class="math inline">\(K\)</span> is determined by the prior distribution and the statistical model. Its stationary distribution is the posterior distribution.</p>

  </body>
</html>
